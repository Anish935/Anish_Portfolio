{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9871fe14-1580-4973-a09a-33ff1d2afd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import packages for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This lets us see all of the columns, preventing Juptyer from redacting them.\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Import packages for data modeling\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,\\\n",
    "f1_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# This is the function that helps plot feature importance\n",
    "from xgboost import plot_importance\n",
    "\n",
    "# This module lets us save our models once we fit them.\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ceea5eb-36ed-47cc-8be3-c60f3e30f984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.3-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy in ./Library/Python/3.9/lib/python/site-packages (from xgboost) (1.26.3)\n",
      "Requirement already satisfied: scipy in ./Library/Python/3.9/lib/python/site-packages (from xgboost) (1.11.4)\n",
      "Downloading xgboost-2.0.3-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccce7431-259e-4160-bffb-8f03c91ccbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import packages for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This lets us see all of the columns, preventing Juptyer from redacting them.\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Import packages for data modeling\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,\\\n",
    "f1_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# This is the function that helps plot feature importance\n",
    "from xgboost import plot_importance\n",
    "\n",
    "# This module lets us save our models once we fit them.\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51c68035-aa4f-4a33-a496-0720f6dea5a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>sessions</th>\n",
       "      <th>drives</th>\n",
       "      <th>total_sessions</th>\n",
       "      <th>n_days_after_onboarding</th>\n",
       "      <th>total_navigations_fav1</th>\n",
       "      <th>total_navigations_fav2</th>\n",
       "      <th>driven_km_drives</th>\n",
       "      <th>duration_minutes_drives</th>\n",
       "      <th>activity_days</th>\n",
       "      <th>driving_days</th>\n",
       "      <th>device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>retained</td>\n",
       "      <td>283</td>\n",
       "      <td>226</td>\n",
       "      <td>296.748273</td>\n",
       "      <td>2276</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "      <td>2628.845068</td>\n",
       "      <td>1985.775061</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>retained</td>\n",
       "      <td>133</td>\n",
       "      <td>107</td>\n",
       "      <td>326.896596</td>\n",
       "      <td>1225</td>\n",
       "      <td>19</td>\n",
       "      <td>64</td>\n",
       "      <td>13715.920550</td>\n",
       "      <td>3160.472914</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>iPhone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>retained</td>\n",
       "      <td>114</td>\n",
       "      <td>95</td>\n",
       "      <td>135.522926</td>\n",
       "      <td>2651</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3059.148818</td>\n",
       "      <td>1610.735904</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>retained</td>\n",
       "      <td>49</td>\n",
       "      <td>40</td>\n",
       "      <td>67.589221</td>\n",
       "      <td>15</td>\n",
       "      <td>322</td>\n",
       "      <td>7</td>\n",
       "      <td>913.591123</td>\n",
       "      <td>587.196542</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>iPhone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>retained</td>\n",
       "      <td>84</td>\n",
       "      <td>68</td>\n",
       "      <td>168.247020</td>\n",
       "      <td>1562</td>\n",
       "      <td>166</td>\n",
       "      <td>5</td>\n",
       "      <td>3950.202008</td>\n",
       "      <td>1219.555924</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "      <td>Android</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID     label  sessions  drives  total_sessions  n_days_after_onboarding  \\\n",
       "0   0  retained       283     226      296.748273                     2276   \n",
       "1   1  retained       133     107      326.896596                     1225   \n",
       "2   2  retained       114      95      135.522926                     2651   \n",
       "3   3  retained        49      40       67.589221                       15   \n",
       "4   4  retained        84      68      168.247020                     1562   \n",
       "\n",
       "   total_navigations_fav1  total_navigations_fav2  driven_km_drives  \\\n",
       "0                     208                       0       2628.845068   \n",
       "1                      19                      64      13715.920550   \n",
       "2                       0                       0       3059.148818   \n",
       "3                     322                       7        913.591123   \n",
       "4                     166                       5       3950.202008   \n",
       "\n",
       "   duration_minutes_drives  activity_days  driving_days   device  \n",
       "0              1985.775061             28            19  Android  \n",
       "1              3160.472914             13            11   iPhone  \n",
       "2              1610.735904             14             8  Android  \n",
       "3               587.196542              7             3   iPhone  \n",
       "4              1219.555924             27            18  Android  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset\n",
    "df0 = pd.read_csv(\"/Users/anish/Desktop/Google data analytics/WAZE/waze_dataset.csv\")\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e39987f4-1059-473f-b459-f2fd4751a432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14999 entries, 0 to 14998\n",
      "Data columns (total 13 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   ID                       14999 non-null  int64  \n",
      " 1   label                    14299 non-null  object \n",
      " 2   sessions                 14999 non-null  int64  \n",
      " 3   drives                   14999 non-null  int64  \n",
      " 4   total_sessions           14999 non-null  float64\n",
      " 5   n_days_after_onboarding  14999 non-null  int64  \n",
      " 6   total_navigations_fav1   14999 non-null  int64  \n",
      " 7   total_navigations_fav2   14999 non-null  int64  \n",
      " 8   driven_km_drives         14999 non-null  float64\n",
      " 9   duration_minutes_drives  14999 non-null  float64\n",
      " 10  activity_days            14999 non-null  int64  \n",
      " 11  driving_days             14999 non-null  int64  \n",
      " 12  device                   14999 non-null  object \n",
      "dtypes: float64(3), int64(8), object(2)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# Copy the df0 dataframe\n",
    "df = df0.copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50d06322-69cf-4243-980c-c31f083721ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anish/Library/Python/3.9/lib/python/site-packages/pandas/core/nanops.py:1010: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1.499900e+04\n",
       "mean              inf\n",
       "std               NaN\n",
       "min      3.022063e+00\n",
       "25%      1.672804e+02\n",
       "50%      3.231459e+02\n",
       "75%      7.579257e+02\n",
       "max               inf\n",
       "Name: km_per_driving_day, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Create `km_per_driving_day` feature\n",
    "df['km_per_driving_day'] = df['driven_km_drives'] / df['driving_days']\n",
    "\n",
    "# 2. Get descriptive stats\n",
    "df['km_per_driving_day'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "449a21f4-6c90-403f-b8f7-68f4e4485709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14999.000000\n",
       "mean       578.963113\n",
       "std       1030.094384\n",
       "min          0.000000\n",
       "25%        136.238895\n",
       "50%        272.889272\n",
       "75%        558.686918\n",
       "max      15420.234110\n",
       "Name: km_per_driving_day, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Convert infinite values to zero\n",
    "df.loc[df['km_per_driving_day']==np.inf, 'km_per_driving_day'] = 0\n",
    "\n",
    "# 2. Confirm that it worked\n",
    "df['km_per_driving_day'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98816657-4848-4e93-b024-2faa3ec07b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14999.000000\n",
       "mean         0.449255\n",
       "std          0.286919\n",
       "min          0.000000\n",
       "25%          0.196221\n",
       "50%          0.423097\n",
       "75%          0.687216\n",
       "max          1.530637\n",
       "Name: percent_sessions_in_last_month, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Create `percent_sessions_in_last_month` feature\n",
    "df['percent_sessions_in_last_month'] = df['sessions'] / df['total_sessions']\n",
    "\n",
    "# 2. Get descriptive stats\n",
    "df['percent_sessions_in_last_month'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8414cf66-0394-4bd6-8bd3-a8c3f0eddee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create `professional_driver` feature\n",
    "df['professional_driver'] = np.where((df['drives'] >= 60) & (df['driving_days'] >= 15), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61eddfe6-dd4c-48fa-9587-78012f280fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create `total_sessions_per_day` feature\n",
    "df['total_sessions_per_day'] = df['total_sessions'] / df['n_days_after_onboarding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0ba79ec-5704-4ec9-8134-6515d011b849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14999.000000\n",
       "mean         0.338698\n",
       "std          1.314333\n",
       "min          0.000298\n",
       "25%          0.051037\n",
       "50%          0.100775\n",
       "75%          0.216269\n",
       "max         39.763874\n",
       "Name: total_sessions_per_day, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get descriptive stats\n",
    "df['total_sessions_per_day'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05642a81-edd5-45cf-a922-d3bb6b4588b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14999.000000\n",
       "mean       190.394608\n",
       "std        334.674026\n",
       "min         72.013095\n",
       "25%         90.706222\n",
       "50%        122.382022\n",
       "75%        193.130119\n",
       "max      23642.920871\n",
       "Name: km_per_hour, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create `km_per_hour` feature\n",
    "df['km_per_hour'] = df['driven_km_drives'] / (df['duration_minutes_drives'] / 60)\n",
    "df['km_per_hour'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88b38599-204c-4c97-83ff-a5405ccf6f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anish/Library/Python/3.9/lib/python/site-packages/pandas/core/nanops.py:1010: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1.499900e+04\n",
       "mean              inf\n",
       "std               NaN\n",
       "min      1.008775e+00\n",
       "25%      3.323065e+01\n",
       "50%      7.488006e+01\n",
       "75%      1.854667e+02\n",
       "max               inf\n",
       "Name: km_per_drive, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create `km_per_drive` feature\n",
    "df['km_per_drive'] = df['driven_km_drives'] / df['drives']\n",
    "df['km_per_drive'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7dbd4f2-a7aa-421c-b96c-46a7125c8b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14999.000000\n",
       "mean       232.817946\n",
       "std        620.622351\n",
       "min          0.000000\n",
       "25%         32.424301\n",
       "50%         72.854343\n",
       "75%        179.347527\n",
       "max      15777.426560\n",
       "Name: km_per_drive, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Convert infinite values to zero\n",
    "df.loc[df['km_per_drive']==np.inf, 'km_per_drive'] = 0\n",
    "\n",
    "# 2. Confirm that it worked\n",
    "df['km_per_drive'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae7299c5-3a75-4cd4-994b-0704745db83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14999.000000\n",
       "mean         1.665439\n",
       "std          8.865666\n",
       "min          0.000000\n",
       "25%          0.203471\n",
       "50%          0.649818\n",
       "75%          1.638526\n",
       "max        777.563629\n",
       "Name: percent_of_drives_to_favorite, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create `percent_of_sessions_to_favorite` feature\n",
    "df['percent_of_drives_to_favorite'] = (\n",
    "    df['total_navigations_fav1'] + df['total_navigations_fav2']) / df['total_sessions']\n",
    "\n",
    "# Get descriptive stats\n",
    "df['percent_of_drives_to_favorite'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd1f7e6c-5f7f-4c66-9682-6fdcc3ea8ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "df = df.dropna(subset=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae942d10-3737-4e35-a0b6-3cf57d41695b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device</th>\n",
       "      <th>device2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14994</th>\n",
       "      <td>iPhone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>Android</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>iPhone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>iPhone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>iPhone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        device  device2\n",
       "14994   iPhone        1\n",
       "14995  Android        0\n",
       "14996   iPhone        1\n",
       "14997   iPhone        1\n",
       "14998   iPhone        1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new `device2` variable\n",
    "df['device2'] = np.where(df['device']=='Android', 0, 1)\n",
    "df[['device', 'device2']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07024700-e401-449e-9290-9477d1c889d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>label2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14994</th>\n",
       "      <td>retained</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>retained</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>retained</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>churned</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>retained</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          label  label2\n",
       "14994  retained       0\n",
       "14995  retained       0\n",
       "14996  retained       0\n",
       "14997   churned       1\n",
       "14998  retained       0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create binary `label2` column\n",
    "df['label2'] = np.where(df['label']=='churned', 1, 0)\n",
    "df[['label', 'label2']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2ac8137-c7f5-43f0-aae6-eb5043d8dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop `ID` column\n",
    "df = df.drop(['ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e728236-6dce-4cf2-93ce-f155a3fe1031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "retained    0.822645\n",
       "churned     0.177355\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get class balance of 'label' col\n",
    "df['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f8bde73-b0e2-4637-b242-1a8804e5d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Isolate X variables\n",
    "X = df.drop(columns=['label', 'label2', 'device'])\n",
    "\n",
    "# 2. Isolate y variable\n",
    "y = df['label2']\n",
    "\n",
    "# 3. Split into train and test sets\n",
    "X_tr, X_test, y_tr, y_test = train_test_split(X, y, stratify=y,\n",
    "                                              test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Split into train and validate sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tr, y_tr, stratify=y_tr,\n",
    "                                                  test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07d1d9f7-ed49-4ff5-8d2c-952569499215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8579\n",
      "2860\n",
      "2860\n"
     ]
    }
   ],
   "source": [
    "for x in [X_train, X_val, X_test]:\n",
    "    print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd51f264-7cb5-4021-a616-c0932ea494c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instantiate the random forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 2. Create a dictionary of hyperparameters to tune\n",
    "cv_params = {'max_depth': [None],\n",
    "             'max_features': [1.0],\n",
    "             'max_samples': [1.0],\n",
    "             'min_samples_leaf': [2],\n",
    "             'min_samples_split': [2],\n",
    "             'n_estimators': [300],\n",
    "             }\n",
    "\n",
    "# 3. Define a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1'}\n",
    "\n",
    "# 4. Instantiate the GridSearchCV object\n",
    "rf_cv = GridSearchCV(rf, cv_params, scoring=scoring, cv=4, refit='recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "154e117f-5cb3-47e2-ada5-cc126879a8cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'scoring' parameter of GridSearchCV must be a str among {'v_measure_score', 'precision_macro', 'top_k_accuracy', 'f1_weighted', 'recall_weighted', 'average_precision', 'neg_mean_squared_error', 'jaccard_macro', 'precision', 'neg_mean_poisson_deviance', 'recall', 'neg_mean_gamma_deviance', 'mutual_info_score', 'roc_auc_ovr', 'explained_variance', 'jaccard_samples', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_log_error', 'matthews_corrcoef', 'homogeneity_score', 'neg_mean_absolute_percentage_error', 'neg_root_mean_squared_error', 'roc_auc_ovo_weighted', 'f1_macro', 'positive_likelihood_ratio', 'jaccard_weighted', 'precision_micro', 'normalized_mutual_info_score', 'balanced_accuracy', 'rand_score', 'adjusted_rand_score', 'neg_median_absolute_error', 'r2', 'recall_macro', 'jaccard', 'max_error', 'f1_samples', 'f1_micro', 'roc_auc_ovr_weighted', 'adjusted_mutual_info_score', 'precision_weighted', 'accuracy', 'completeness_score', 'recall_micro', 'roc_auc_ovo', 'jaccard_micro', 'f1', 'roc_auc', 'precision_samples', 'neg_negative_likelihood_ratio', 'neg_brier_score', 'recall_samples', 'fowlkes_mallows_score'}, a callable, an instance of 'list', an instance of 'tuple', an instance of 'dict' or None. Got {'f1', 'precision', 'accuracy', 'recall'} instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:1145\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1141\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[1;32m   1142\u001b[0m )\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[0;32m-> 1145\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:638\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    631\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \n\u001b[1;32m    633\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 638\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/_param_validation.py:96\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m     )\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'scoring' parameter of GridSearchCV must be a str among {'v_measure_score', 'precision_macro', 'top_k_accuracy', 'f1_weighted', 'recall_weighted', 'average_precision', 'neg_mean_squared_error', 'jaccard_macro', 'precision', 'neg_mean_poisson_deviance', 'recall', 'neg_mean_gamma_deviance', 'mutual_info_score', 'roc_auc_ovr', 'explained_variance', 'jaccard_samples', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_log_error', 'matthews_corrcoef', 'homogeneity_score', 'neg_mean_absolute_percentage_error', 'neg_root_mean_squared_error', 'roc_auc_ovo_weighted', 'f1_macro', 'positive_likelihood_ratio', 'jaccard_weighted', 'precision_micro', 'normalized_mutual_info_score', 'balanced_accuracy', 'rand_score', 'adjusted_rand_score', 'neg_median_absolute_error', 'r2', 'recall_macro', 'jaccard', 'max_error', 'f1_samples', 'f1_micro', 'roc_auc_ovr_weighted', 'adjusted_mutual_info_score', 'precision_weighted', 'accuracy', 'completeness_score', 'recall_micro', 'roc_auc_ovo', 'jaccard_micro', 'f1', 'roc_auc', 'precision_samples', 'neg_negative_likelihood_ratio', 'neg_brier_score', 'recall_samples', 'fowlkes_mallows_score'}, a callable, an instance of 'list', an instance of 'tuple', an instance of 'dict' or None. Got {'f1', 'precision', 'accuracy', 'recall'} instead."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "085d0df8-76bb-428e-a4a5-f0dcbfc058b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_score_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Examine best score\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrf_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_score_\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_score_'"
     ]
    }
   ],
   "source": [
    "# Examine best score\n",
    "rf_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdd86f4b-a39b-42a4-967f-c54cac4f073d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Examine best hyperparameter combo\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrf_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params_\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "# Examine best hyperparameter combo\n",
    "rf_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b51691ba-e28a-4a0a-93b1-615caba8b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_results(model_name:str, model_object, metric:str):\n",
    "    '''\n",
    "    Arguments:\n",
    "        model_name (string): what you want the model to be called in the output table\n",
    "        model_object: a fit GridSearchCV object\n",
    "        metric (string): precision, recall, f1, or accuracy\n",
    "\n",
    "    Returns a pandas df with the F1, recall, precision, and accuracy scores\n",
    "    for the model with the best mean 'metric' score across all validation folds.\n",
    "    '''\n",
    "\n",
    "    # Create dictionary that maps input metric to actual metric name in GridSearchCV\n",
    "    metric_dict = {'precision': 'mean_test_precision',\n",
    "                   'recall': 'mean_test_recall',\n",
    "                   'f1': 'mean_test_f1',\n",
    "                   'accuracy': 'mean_test_accuracy',\n",
    "                   }\n",
    "\n",
    "    # Get all the results from the CV and put them in a df\n",
    "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
    "\n",
    "    # Isolate the row of the df with the max(metric) score\n",
    "    best_estimator_results = cv_results.iloc[cv_results[metric_dict[metric]].idxmax(), :]\n",
    "\n",
    "    # Extract accuracy, precision, recall, and f1 score from that row\n",
    "    f1 = best_estimator_results.mean_test_f1\n",
    "    recall = best_estimator_results.mean_test_recall\n",
    "    precision = best_estimator_results.mean_test_precision\n",
    "    accuracy = best_estimator_results.mean_test_accuracy\n",
    "\n",
    "    # Create table of results\n",
    "    table = pd.DataFrame({'model': [model_name],\n",
    "                          'precision': [precision],\n",
    "                          'recall': [recall],\n",
    "                          'F1': [f1],\n",
    "                          'accuracy': [accuracy],\n",
    "                          },\n",
    "                         )\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92aa71f8-c272-41ce-8d1d-a880acf0a4ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'cv_results_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmake_results\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRF cv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrf_cv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m results\n",
      "Cell \u001b[0;32mIn[33], line 20\u001b[0m, in \u001b[0;36mmake_results\u001b[0;34m(model_name, model_object, metric)\u001b[0m\n\u001b[1;32m     13\u001b[0m metric_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_test_precision\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_test_recall\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     15\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_test_f1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_test_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m                }\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Get all the results from the CV and put them in a df\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mmodel_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv_results_\u001b[49m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Isolate the row of the df with the max(metric) score\u001b[39;00m\n\u001b[1;32m     23\u001b[0m best_estimator_results \u001b[38;5;241m=\u001b[39m cv_results\u001b[38;5;241m.\u001b[39miloc[cv_results[metric_dict[metric]]\u001b[38;5;241m.\u001b[39midxmax(), :]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'cv_results_'"
     ]
    }
   ],
   "source": [
    "results = make_results('RF cv', rf_cv, 'recall')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "172c6dbc-4a74-4478-a6b8-478b1d47d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instantiate the XGBoost classifier\n",
    "xgb = XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# 2. Create a dictionary of hyperparameters to tune\n",
    "cv_params = {'max_depth': [6, 12],\n",
    "             'min_child_weight': [3, 5],\n",
    "             'learning_rate': [0.01, 0.1],\n",
    "             'n_estimators': [300]\n",
    "             }\n",
    "\n",
    "# 3. Define a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1'}\n",
    "\n",
    "# 4. Instantiate the GridSearchCV object\n",
    "xgb_cv = GridSearchCV(xgb, cv_params, scoring=scoring, cv=4, refit='recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b85b7f2-625d-43a2-ba10-662b618e3380",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'scoring' parameter of GridSearchCV must be a str among {'v_measure_score', 'precision_macro', 'top_k_accuracy', 'f1_weighted', 'recall_weighted', 'average_precision', 'neg_mean_squared_error', 'jaccard_macro', 'precision', 'neg_mean_poisson_deviance', 'recall', 'neg_mean_gamma_deviance', 'mutual_info_score', 'roc_auc_ovr', 'explained_variance', 'jaccard_samples', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_log_error', 'matthews_corrcoef', 'homogeneity_score', 'neg_mean_absolute_percentage_error', 'neg_root_mean_squared_error', 'roc_auc_ovo_weighted', 'f1_macro', 'positive_likelihood_ratio', 'jaccard_weighted', 'precision_micro', 'normalized_mutual_info_score', 'balanced_accuracy', 'rand_score', 'adjusted_rand_score', 'neg_median_absolute_error', 'r2', 'recall_macro', 'jaccard', 'max_error', 'f1_samples', 'f1_micro', 'roc_auc_ovr_weighted', 'adjusted_mutual_info_score', 'precision_weighted', 'accuracy', 'completeness_score', 'recall_micro', 'roc_auc_ovo', 'jaccard_micro', 'f1', 'roc_auc', 'precision_samples', 'neg_negative_likelihood_ratio', 'neg_brier_score', 'recall_samples', 'fowlkes_mallows_score'}, a callable, an instance of 'list', an instance of 'tuple', an instance of 'dict' or None. Got {'f1', 'precision', 'accuracy', 'recall'} instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:1145\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1141\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[1;32m   1142\u001b[0m )\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[0;32m-> 1145\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:638\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    631\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \n\u001b[1;32m    633\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 638\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/_param_validation.py:96\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m     )\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'scoring' parameter of GridSearchCV must be a str among {'v_measure_score', 'precision_macro', 'top_k_accuracy', 'f1_weighted', 'recall_weighted', 'average_precision', 'neg_mean_squared_error', 'jaccard_macro', 'precision', 'neg_mean_poisson_deviance', 'recall', 'neg_mean_gamma_deviance', 'mutual_info_score', 'roc_auc_ovr', 'explained_variance', 'jaccard_samples', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_log_error', 'matthews_corrcoef', 'homogeneity_score', 'neg_mean_absolute_percentage_error', 'neg_root_mean_squared_error', 'roc_auc_ovo_weighted', 'f1_macro', 'positive_likelihood_ratio', 'jaccard_weighted', 'precision_micro', 'normalized_mutual_info_score', 'balanced_accuracy', 'rand_score', 'adjusted_rand_score', 'neg_median_absolute_error', 'r2', 'recall_macro', 'jaccard', 'max_error', 'f1_samples', 'f1_micro', 'roc_auc_ovr_weighted', 'adjusted_mutual_info_score', 'precision_weighted', 'accuracy', 'completeness_score', 'recall_micro', 'roc_auc_ovo', 'jaccard_micro', 'f1', 'roc_auc', 'precision_samples', 'neg_negative_likelihood_ratio', 'neg_brier_score', 'recall_samples', 'fowlkes_mallows_score'}, a callable, an instance of 'list', an instance of 'tuple', an instance of 'dict' or None. Got {'f1', 'precision', 'accuracy', 'recall'} instead."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c70710e6-7797-4a1c-b3bc-684f9dede818",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_score_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Examine best score\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mxgb_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_score_\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_score_'"
     ]
    }
   ],
   "source": [
    "# Examine best score\n",
    "xgb_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56eddd10-9dfa-4c4a-b0bf-30b8f33dd825",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Examine best parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mxgb_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params_\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "# Examine best parameters\n",
    "xgb_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab962b03-9fce-4871-8d4e-8b6ed9841922",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'cv_results_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Call 'make_results()' on the GridSearch object\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m xgb_cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mmake_results\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mXGB cv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxgb_cv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([results, xgb_cv_results], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m results\n",
      "Cell \u001b[0;32mIn[33], line 20\u001b[0m, in \u001b[0;36mmake_results\u001b[0;34m(model_name, model_object, metric)\u001b[0m\n\u001b[1;32m     13\u001b[0m metric_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_test_precision\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_test_recall\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     15\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_test_f1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_test_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m                }\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Get all the results from the CV and put them in a df\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mmodel_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv_results_\u001b[49m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Isolate the row of the df with the max(metric) score\u001b[39;00m\n\u001b[1;32m     23\u001b[0m best_estimator_results \u001b[38;5;241m=\u001b[39m cv_results\u001b[38;5;241m.\u001b[39miloc[cv_results[metric_dict[metric]]\u001b[38;5;241m.\u001b[39midxmax(), :]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'cv_results_'"
     ]
    }
   ],
   "source": [
    "# Call 'make_results()' on the GridSearch object\n",
    "xgb_cv_results = make_results('XGB cv', xgb_cv, 'recall')\n",
    "results = pd.concat([results, xgb_cv_results], axis=0)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8f1cfef-99fe-43af-a1c6-5c6adb0eea07",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use random forest model to predict on validation data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m rf_val_preds \u001b[38;5;241m=\u001b[39m \u001b[43mrf_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "# Use random forest model to predict on validation data\n",
    "rf_val_preds = rf_cv.best_estimator_.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f2294d7-b21a-4f9f-b997-a6aa58e3be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_scores(model_name:str, preds, y_test_data):\n",
    "    '''\n",
    "    Generate a table of test scores.\n",
    "\n",
    "    In:\n",
    "        model_name (string): Your choice: how the model will be named in the output table\n",
    "        preds: numpy array of test predictions\n",
    "        y_test_data: numpy array of y_test data\n",
    "\n",
    "    Out:\n",
    "        table: a pandas df of precision, recall, f1, and accuracy scores for your model\n",
    "    '''\n",
    "    accuracy = accuracy_score(y_test_data, preds)\n",
    "    precision = precision_score(y_test_data, preds)\n",
    "    recall = recall_score(y_test_data, preds)\n",
    "    f1 = f1_score(y_test_data, preds)\n",
    "\n",
    "    table = pd.DataFrame({'model': [model_name],\n",
    "                          'precision': [precision],\n",
    "                          'recall': [recall],\n",
    "                          'F1': [f1],\n",
    "                          'accuracy': [accuracy]\n",
    "                          })\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "602eb0e7-d7d6-4317-99fa-fd9cdaa871d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf_val_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get validation scores for RF model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m rf_val_scores \u001b[38;5;241m=\u001b[39m get_test_scores(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRF val\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mrf_val_preds\u001b[49m, y_val)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Append to the results table\u001b[39;00m\n\u001b[1;32m      5\u001b[0m results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([results, rf_val_scores], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rf_val_preds' is not defined"
     ]
    }
   ],
   "source": [
    "# Get validation scores for RF model\n",
    "rf_val_scores = get_test_scores('RF val', rf_val_preds, y_val)\n",
    "\n",
    "# Append to the results table\n",
    "results = pd.concat([results, rf_val_scores], axis=0)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b1376f7-59ba-4ebe-841f-69108a51081b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use XGBoost model to predict on validation data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m xgb_val_preds \u001b[38;5;241m=\u001b[39m \u001b[43mxgb_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Get validation scores for XGBoost model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m xgb_val_scores \u001b[38;5;241m=\u001b[39m get_test_scores(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGB val\u001b[39m\u001b[38;5;124m'\u001b[39m, xgb_val_preds, y_val)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "# Use XGBoost model to predict on validation data\n",
    "xgb_val_preds = xgb_cv.best_estimator_.predict(X_val)\n",
    "\n",
    "# Get validation scores for XGBoost model\n",
    "xgb_val_scores = get_test_scores('XGB val', xgb_val_preds, y_val)\n",
    "\n",
    "# Append to the results table\n",
    "results = pd.concat([results, xgb_val_scores], axis=0)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cfdc395c-4bc5-454f-bbce-97c720ebf3f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use XGBoost model to predict on test data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m xgb_test_preds \u001b[38;5;241m=\u001b[39m \u001b[43mxgb_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Get test scores for XGBoost model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m xgb_test_scores \u001b[38;5;241m=\u001b[39m get_test_scores(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGB test\u001b[39m\u001b[38;5;124m'\u001b[39m, xgb_test_preds, y_test)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "# Use XGBoost model to predict on test data\n",
    "xgb_test_preds = xgb_cv.best_estimator_.predict(X_test)\n",
    "\n",
    "# Get test scores for XGBoost model\n",
    "xgb_test_scores = get_test_scores('XGB test', xgb_test_preds, y_test)\n",
    "\n",
    "# Append to the results table\n",
    "results = pd.concat([results, xgb_test_scores], axis=0)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2ee2af4-515e-4e81-86a7-2d74272778d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_test_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate array of values for confusion matrix\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(y_test, \u001b[43mxgb_test_preds\u001b[49m, labels\u001b[38;5;241m=\u001b[39mxgb_cv\u001b[38;5;241m.\u001b[39mclasses_)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Plot confusion matrix\u001b[39;00m\n\u001b[1;32m      5\u001b[0m disp \u001b[38;5;241m=\u001b[39m ConfusionMatrixDisplay(confusion_matrix\u001b[38;5;241m=\u001b[39mcm,\n\u001b[1;32m      6\u001b[0m                              display_labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretained\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchurned\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xgb_test_preds' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate array of values for confusion matrix\n",
    "cm = confusion_matrix(y_test, xgb_test_preds, labels=xgb_cv.classes_)\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                             display_labels=['retained', 'churned'])\n",
    "disp.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd48befa-f7f9-417c-9efb-fc50e3dd849f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot precision-recall curve\u001b[39;00m\n\u001b[1;32m      2\u001b[0m display \u001b[38;5;241m=\u001b[39m PrecisionRecallDisplay\u001b[38;5;241m.\u001b[39mfrom_estimator(\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mxgb_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m, X_test, y_test, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m     )\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision-recall curve, XGBoost model\u001b[39m\u001b[38;5;124m'\u001b[39m);\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "# Plot precision-recall curve\n",
    "display = PrecisionRecallDisplay.from_estimator(\n",
    "    xgb_cv.best_estimator_, X_test, y_test, name='XGBoost'\n",
    "    )\n",
    "plt.title('Precision-recall curve, XGBoost model');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "981b558e-30ee-4c77-8b63-3b805c25bdc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_importance(\u001b[43mxgb_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m);\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "plot_importance(xgb_cv.best_estimator_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e5e3ebb-6346-4f1c-b02d-8f37da026d5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get predicted probabilities on the test data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predicted_probabilities \u001b[38;5;241m=\u001b[39m \u001b[43mxgb_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)\n\u001b[1;32m      3\u001b[0m predicted_probabilities\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "# Get predicted probabilities on the test data\n",
    "predicted_probabilities = xgb_cv.best_estimator_.predict_proba(X_test)\n",
    "predicted_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "820f3d5f-e5b9-47bb-9a3e-ff703a73bee4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_probabilities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create a list of just the second column values (probability of target)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m probs \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpredicted_probabilities\u001b[49m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create an array of new predictions that assigns a 1 to any value >= 0.4\u001b[39;00m\n\u001b[1;32m      5\u001b[0m new_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m probs])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predicted_probabilities' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a list of just the second column values (probability of target)\n",
    "probs = [x[1] for x in predicted_probabilities]\n",
    "\n",
    "# Create an array of new predictions that assigns a 1 to any value >= 0.4\n",
    "new_preds = np.array([1 if x >= 0.4 else 0 for x in probs])\n",
    "new_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a58fc77-2cf5-4d68-9dc3-eba77c32471c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get evaluation metrics for when the threshold is 0.4\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m get_test_scores(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGB, threshold = 0.4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mnew_preds\u001b[49m, y_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_preds' is not defined"
     ]
    }
   ],
   "source": [
    "# Get evaluation metrics for when the threshold is 0.4\n",
    "get_test_scores('XGB, threshold = 0.4', new_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "38908cdf-8161-4933-b91d-09088446f501",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresults\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b49056c-588e-44f4-ab9d-1166bdca661a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
